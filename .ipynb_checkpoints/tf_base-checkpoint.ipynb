{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一个随机数（标量）\n",
    "random_float = tf.random.uniform(shape=())\n",
    "\n",
    "#定义一个有两个元素的零向量\n",
    "zero_vector = tf.zeros(shape=(2))\n",
    "\n",
    "#定义两个2×2的常量举证\n",
    "A = tf.constant([[1.,2.],[3.,4.]])\n",
    "B = tf.constant([[5.,6.],[7.,8.]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "<dtype: 'float32'>\n",
      "<bound method _EagerTensorBase.numpy of <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
      "array([[1., 2.],\n",
      "       [3., 4.]], dtype=float32)>>\n"
     ]
    }
   ],
   "source": [
    "print(A.shape)\n",
    "print(A.dtype)\n",
    "print(A.numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 6.  8.]\n",
      " [10. 12.]], shape=(2, 2), dtype=float32) tf.Tensor(\n",
      "[[19. 22.]\n",
      " [43. 50.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "C = tf.add(A,B)\n",
    "D = tf.matmul(A,B)\n",
    "print(C,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#计算函数 y(x) = x^2 在 x = 3 时的导数：\n",
    "x = tf.Variable(initial_value=3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.square(x)\n",
    "y_grad = tape.gradient(y,x)\n",
    "print(y)\n",
    "print(y_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里 x 是一个初始化为 3 的 变量 （Variable），使用 tf.Variable() 声明。与普通张量一样，变量同样具有形状、类型和值三种属性。使用变量需要有一个初始化过程，可以通过在 tf.Variable() 中指定 initial_value 参数来指定初始值。这里将变量 x 初始化为 3. 1。变量与普通张量的一个重要区别是其默认能够被 TensorFlow 的自动求导机制所求导，因此往往被用于定义机器学习模型的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.GradientTape() 是一个自动求导的记录器。只要进入了 with tf.GradientTape() as tape 的上下文环境，则在该环境中计算步骤都会被自动记录。比如在上面的示例中，计算步骤 y = tf.square(x) 即被自动记录。离开上下文环境后，记录将停止，但记录器 tape 依然可用，因此可以通过 y_grad = tape.gradient(y, x) 求张量 y 对变量 x 的导数。\n",
    "\n",
    "在机器学习中，更加常见的是对多元函数求偏导数，以及对向量或矩阵的求导。这些对于 TensorFlow 也不在话下。以下代码展示了如何使用 tf.GradientTape() 计算函数 L(w, b) = \\|Xw + b - y\\|^2 在 w = (1, 2)^T, b = 1 时分别对 w, b 的偏导数。其中 X = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix},  y = \\begin{bmatrix} 1 \\\\ 2\\end{bmatrix}。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(125.0, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 70.]\n",
      " [100.]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor(30.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant([[1.,2.],[3.,4.]])\n",
    "y = tf.constant([[1.],[2.]])\n",
    "w = tf.Variable(initial_value=[[1.],[2.]])\n",
    "b = tf.Variable(initial_value=1.)\n",
    "with tf.GradientTape() as tape:\n",
    "    L = tf.reduce_sum(tf.square(tf.matmul(X,w) + b - y))\n",
    "w_grad, b_grad = tape.gradient(L,[w,b])\n",
    "print(L)\n",
    "print(w_grad)\n",
    "print(b_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里， tf.square() 操作代表对输入张量的每一个元素求平方，不改变张量形状。 tf.reduce_sum() 操作代表对输入张量的所有元素求和，输出一个形状为空的纯量张量（可以通过 axis 参数来指定求和的维度，不指定则默认对所有元素求和）。TensorFlow 中有大量的张量操作 API，包括数学运算、张量形状操作（如 tf.reshape()）、切片和连接（如 tf.concat()）等多种类型，可以通过查阅 TensorFlow 的官方 API 文档 2 来进一步了解。\n",
    "\n",
    "从输出可见，TensorFlow 帮助我们计算出了\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑一个实际问题，某城市在 2013 年 - 2017 年的房价如下表所示：\n",
    "\n",
    "年份\n",
    "\n",
    "2013\n",
    "\n",
    "2014\n",
    "\n",
    "2015\n",
    "\n",
    "2016\n",
    "\n",
    "2017\n",
    "\n",
    "房价\n",
    "\n",
    "12000\n",
    "\n",
    "14000\n",
    "\n",
    "15000\n",
    "\n",
    "16500\n",
    "\n",
    "17500\n",
    "\n",
    "现在，我们希望通过对该数据进行线性回归，即使用线性模型 y = ax + b 来拟合上述数据，此处 a 和 b 是待求的参数。\n",
    "\n",
    "首先，我们定义数据，进行基本的归一化操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_raw = np.array([2013, 2014, 2015, 2016, 2017], dtype=np.float32)\n",
    "y_raw = np.array([12000, 14000, 15000, 16500, 17500], dtype=np.float32)\n",
    "X = (X_raw - X_raw.min()) / (X_raw.max() - X_raw.min())\n",
    "y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们使用梯度下降方法来求线性模型中两个参数 a 和 b 的值 3。\n",
    "\n",
    "回顾机器学习的基础知识，对于多元函数 f(x) 求局部极小值，梯度下降 的过程如下：\n",
    "\n",
    "初始化自变量为 x_0 ， k=0\n",
    "\n",
    "迭代进行下列步骤直到满足收敛条件：\n",
    "\n",
    "求函数 f(x) 关于自变量的梯度 ▽f(x_k)\n",
    "\n",
    "更新自变量： x_{k+1} = x_{k} - 学习率▽f(x_k) 。这里学习率（也就是梯度下降一次迈出的 “步子” 大小）\n",
    "\n",
    "k <- k+1\n",
    "\n",
    "接下来，我们考虑如何使用程序来实现梯度下降方法，求得线性回归的解 min_{a, b} L(a, b) = ∑n,i=1(ax_i + b - y_i)^2 。\n",
    "\n",
    "NumPy 下的线性回归 \n",
    "机器学习模型的实现并不是 TensorFlow 的专利。事实上，对于简单的模型，即使使用常规的科学计算库或者工具也可以求解。在这里，我们使用 NumPy 这一通用的科学计算库来实现梯度下降方法。NumPy 提供了多维数组支持，可以表示向量、矩阵以及更高维的张量。同时，也提供了大量支持在多维数组上进行操作的函数（比如下面的 np.dot() 是求内积， np.sum() 是求和）。在这方面，NumPy 和 MATLAB 比较类似。在以下代码中，我们手工求损失函数关于参数 a 和 b 的偏导数 4，并使用梯度下降法反复迭代，最终获得 a 和 b 的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9763702027872223\n",
      "0.057564988337455304\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "b = 0\n",
    "num_epoch = 10000\n",
    "learning_rate = 5e-4\n",
    "for e in range(num_epoch):\n",
    "    # 手动计算损失函数关于自变量（模型参数）的梯度\n",
    "    y_pred = a * X + b\n",
    "    grad_a, grad_b = 2 * (y_pred - y).dot(X), 2 * (y_pred - y).sum()\n",
    "     # 更新参数\n",
    "    a,b = a - learning_rate * grad_a, b - learning_rate * grad_b\n",
    "print(a)\n",
    "print(b)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而，你或许已经可以注意到，使用常规的科学计算库实现机器学习模型有两个痛点：\n",
    "\n",
    "经常需要手工求函数关于参数的偏导数。如果是简单的函数或许还好，但一旦函数的形式变得复杂（尤其是深度学习模型），手工求导的过程将变得非常痛苦，甚至不可行。\n",
    "\n",
    "经常需要手工根据求导的结果更新参数。这里使用了最基础的梯度下降方法，因此参数的更新还较为容易。但如果使用更加复杂的参数更新方法（例如 Adam 或者 Adagrad），这个更新过程的编写同样会非常繁杂。\n",
    "\n",
    "而 TensorFlow 等深度学习框架的出现很大程度上解决了这些痛点，为机器学习模型的实现带来了很大的便利。\n",
    "\n",
    "TensorFlow 下的线性回归 \n",
    "TensorFlow 的 即时执行模式 5 与上述 NumPy 的运行方式十分类似，然而提供了更快速的运算（GPU 支持）、自动求导、优化器等一系列对深度学习非常重要的功能。以下展示了如何使用 TensorFlow 计算线性回归。可以注意到，程序的结构和前述 NumPy 的实现非常类似。这里，TensorFlow 帮助我们做了两件重要的工作：\n",
    "\n",
    "使用 tape.gradient(ys, xs) 自动计算梯度；\n",
    "\n",
    "使用 optimizer.apply_gradients(grads_and_vars) 自动更新模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(X)\n",
    "y = tf.constant(y)\n",
    "a = tf.Variable(initial_value=0.)\n",
    "b = tf.Variable(initial_value=0.)\n",
    "variable = [a,b]\n",
    "num_epoch = 10000\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate = 5e-4)\n",
    "for e in range(num_epoch):\n",
    "     # 使用tf.GradientTape()记录损失函数的梯度信息\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = a *X + b\n",
    "        loss = tf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
